{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuser -v /dev/nvidia*\n",
    "#sudo pkill -f ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import data_processor\n",
    "import time\n",
    "import os\n",
    "import io\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint, Callback\n",
    "\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam, SGD\n",
    "import keras.backend as K\n",
    "\n",
    "from adamw import AdamW\n",
    "from keras.models import load_model, model_from_json\n",
    "\n",
    "from model import EAST_model\n",
    "from losses import dice_loss, rbox_loss\n",
    "import data_processor\n",
    "from data_processor import restore_rectangle\n",
    "import lanms\n",
    "import random\n",
    "import cv2\n",
    "import gc \n",
    "from for_train import *\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input_size', type=int, default=512) # input size for training of the network\n",
    "parser.add_argument('--batch_size', type=int, default=8) # batch size for training\n",
    "parser.add_argument('--nb_workers', type=int, default=4) # number of processes to spin up when using process based threading, as defined in https://keras.io/models/model/#fit_generator\n",
    "parser.add_argument('--init_learning_rate', type=float, default=0.01) # initial learning rate\n",
    "parser.add_argument('--lr_decay_rate', type=float, default=0.94) # decay rate for the learning rate\n",
    "parser.add_argument('--lr_decay_steps', type=int, default=130) # number of steps after which the learning rate is decayed by decay rate\n",
    "parser.add_argument('--max_epochs', type=int, default=800) # maximum number of epochs\n",
    "parser.add_argument('--gpu_list', type=str, default='0') # list of gpus to use\n",
    "parser.add_argument('--checkpoint_path', type=str, default='tmp/east_resnet_50_rbox') # path to a directory to save model checkpoints during training\n",
    "parser.add_argument('--save_checkpoint_epochs', type=int, default=5) # period at which checkpoints are saved (defaults to every 10 epochs)\n",
    "parser.add_argument('--training_data_path', type=str, default='../data/train/augmented_images') # path to training data\n",
    "parser.add_argument('--validation_data_path', type=str, default='../data/val/augmented_images') # path to validation data\n",
    "parser.add_argument('--max_image_large_side', type=int, default=1280) # maximum size of the large side of a training image before cropping a patch for training\n",
    "parser.add_argument('--max_text_size', type=int, default=800) # maximum size of a text instance in an image; image resized if this limit is exceeded\n",
    "parser.add_argument('--min_text_size', type=int, default=10) # minimum size of a text instance; if smaller, then it is ignored during training\n",
    "parser.add_argument('--min_crop_side_ratio', type=float, default=0.1) # the minimum ratio of min(H, W), the smaller side of the image, when taking a random crop from thee input image\n",
    "parser.add_argument('--geometry', type=str, default='RBOX') # geometry type to be used; only RBOX is implemented now, but the original paper also uses QUAD\n",
    "parser.add_argument('--suppress_warnings_and_error_messages', type=bool, default=True) # whether to show error messages and warnings during training (some error messages during training are expected to appear because of the way patches for training are created)\n",
    "FLAGS, unknown = parser.parse_known_args()\n",
    "gpus = list(range(len(FLAGS.gpu_list.split(','))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESIZE_FACTOR = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split in Train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FOLDER = \"../data/train/augmented_images/\"\n",
    "VAL_FOLDER = \"../data/val/augmented_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list_orig = [f[:-4] for f in os.listdir(VAL_FOLDER) if f[-4:] ==\".png\"]\n",
    "for i in range(len(val_list_orig)):\n",
    "    shutil.move(VAL_FOLDER + val_list_orig[i] + \".png\", TRAIN_FOLDER + \n",
    "                val_list_orig[i] + \".png\")\n",
    "    shutil.move(VAL_FOLDER + val_list_orig[i] + \".txt\", TRAIN_FOLDER + \n",
    "                val_list_orig[i] + \".txt\")\n",
    "\n",
    "del val_list_orig\n",
    "    \n",
    "f_list = [f[:-4] for f in os.listdir(TRAIN_FOLDER) if f[-4:] ==\".png\"]\n",
    "val_list = random.sample(f_list, 50)\n",
    "\n",
    "for i in range(len(val_list)):\n",
    "    shutil.move(TRAIN_FOLDER + val_list[i] + \".png\", VAL_FOLDER +\n",
    "                val_list[i] + \".png\")\n",
    "    shutil.move(TRAIN_FOLDER + val_list[i] + \".txt\", VAL_FOLDER +\n",
    "                val_list[i] + \".txt\")\n",
    "\n",
    "f_list_train = [f[:-4] for f in os.listdir(TRAIN_FOLDER) if f[-4:] ==\".png\"]\n",
    "f_list_val = [f[:-4] for f in os.listdir(VAL_FOLDER) if f[-4:] ==\".png\"]\n",
    "\n",
    "print(len(f_list_train))\n",
    "print(len(f_list_val))\n",
    "\n",
    "del f_list\n",
    "del val_list\n",
    "del f_list_train\n",
    "del f_list_val\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = data_processor.load_data(FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = data_processor.generator(FLAGS)\n",
    "train_samples_count = data_processor.count_samples(FLAGS)\n",
    "train_samples_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "east = EAST_model(FLAGS.input_size)\n",
    "parallel_model = east.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_model.load_weights(FLAGS.checkpoint_path + \"/model-90.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_decay(epoch):\n",
    "    return FLAGS.init_learning_rate * np.power(FLAGS.lr_decay_rate, epoch // FLAGS.lr_decay_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = east.model.to_json()\n",
    "with open(FLAGS.checkpoint_path + '/model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = FLAGS.gpu_list\n",
    "\n",
    "# check if checkpoint path exists\n",
    "if not os.path.exists(FLAGS.checkpoint_path):\n",
    "    os.mkdir(FLAGS.checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_map_loss_weight = K.variable(0.01, name='score_map_loss_weight')\n",
    "small_text_weight = K.variable(0., name='small_text_weight')\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_decay)\n",
    "ckpt = CustomModelCheckpoint(model=parallel_model, path=FLAGS.checkpoint_path + '/model-{epoch:02d}.h5', period=FLAGS.save_checkpoint_epochs, save_weights_only=True)\n",
    "tb = CustomTensorBoard(log_dir=FLAGS.checkpoint_path + '/train', score_map_loss_weight=score_map_loss_weight, small_text_weight=small_text_weight, data_generator=train_data_generator, write_graph=True)\n",
    "small_text_weight_callback = SmallTextWeight(small_text_weight)\n",
    "validation_evaluator = ValidationEvaluator(val_data, validation_log_dir=FLAGS.checkpoint_path + '/val')\n",
    "callbacks = [lr_scheduler,ckpt, tb, small_text_weight_callback, validation_evaluator]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in  range(10):\n",
    "    #NB_LAYERS_TO_TRAIN = 5 + (5*i)    \n",
    "    #INIT = 20 + (i*10)\n",
    "NB_LAYERS_TO_TRAIN = 100\n",
    "INIT = 25\n",
    "for layer in parallel_model.layers[:-NB_LAYERS_TO_TRAIN]:\n",
    "    layer.trainable = False\n",
    "for layer in parallel_model.layers[-NB_LAYERS_TO_TRAIN:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "opt = AdamW(FLAGS.init_learning_rate)\n",
    "\n",
    "parallel_model.compile(loss=[dice_loss(east.overly_small_text_region_training_mask, \n",
    "                                       east.text_region_boundary_training_mask, score_map_loss_weight, \n",
    "                                       small_text_weight), \n",
    "                             rbox_loss(east.overly_small_text_region_training_mask, \n",
    "                                       east.text_region_boundary_training_mask, small_text_weight, \n",
    "                                       east.target_score_map)], loss_weights=[1., 1.], \n",
    "                       optimizer=opt)\n",
    "\n",
    "history = parallel_model.fit_generator(train_data_generator, epochs=800,\n",
    "                                       initial_epoch = 0,\n",
    "                                       steps_per_epoch=train_samples_count/FLAGS.batch_size,\n",
    "                                       use_multiprocessing=True, \n",
    "                                       max_queue_size=1, callbacks=callbacks, verbose=1, \n",
    "                                       workers=FLAGS.nb_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copyfile(FLAGS.checkpoint_path + '/model.json', \"../model_east.json\")\n",
    "shutil.copyfile(FLAGS.checkpoint_path + \"/model-90.h5\", \"../weights_east.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
